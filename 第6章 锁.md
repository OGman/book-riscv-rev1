包括xv6在内的大部分内核，都交替执行不同的活动。交替执行的一个来源是多处理器硬件：计算机中有多个CPU独立执行，例如xv6的RISC-V。多CPU共享物理RAM，xv6利用共享，维护所有CPU都需要读写的数据结构。这种共享提高了一个CPU在更新一个数据结构到一半时，另一个CPU读取它的可能性。也提高了多个CPU同时更新同一个数据的可能性。没有仔细的设计，这种并行访问很可能导致错误的结果或者造成数据结构的破坏。即使是单处理器，内核也可能在不同线程之间切换CPU，导致它们的执行相互交替。最后还有一种情况，当设备终端handler与被打断的代码修改相同数据，如果中断在错误的时间出现，就可能造成数据的破坏。并行指的是多条指令流相互交替执行的情况，原因可以是多处理器、线程切换或者中断。

内核中充满了并发访问的数据。举个例子，两个CPU可以同时调用kalloc，一次同时从空闲页链表头部弹出页。内核设计者喜欢高并发，因为这样能通过并行获取更高的性能，同时也能够提升响应性能。然而，这样做的结果是，内核设计者们必须付出很多努力，来保证并发之下的正确性。有很多方法能写出正确的并发代码，有些比别的更容易理解。保证并发之下正确的策略与支持它们的抽象被称为并发控制技术。

xv6在不同的情境之下，使用多种并发控制技术；可以用的还用更多。本章主要介绍一种广泛使用的技术：锁。锁提供了互斥性（mutual exclusion, mutex就这么来的），保证在同一时间只有一个CPU持有这把锁。如果程序员将每个共享数据项关联一把锁，代码在使用一个项的时候总是持有关联的锁，那就能保证这个数据项在同一时间只被一个CPU使用。在这种情况下，我们就说锁保护了这个数据项。尽管锁是一种容易理解的并发控制机制，但它的缺点在于，它的使用会降低性能，因为从本质上说，锁把并行操作串行化了。

本章的其他部分解释了为什么xv6需要锁，xv6怎样实现锁，以及怎样使用锁。

interleave  simultaneously  concurrency  parallelism  mutual exclusion

# 6.1 竞争条件（Race Condition）

为了阐明为什么我们需要锁，我们来看一个例子，在不同CPU上运行的两个进程都调用了wait。wait释放了子进程的内存。因此，在每个CPU上，内核都会调用kfree来释放子进程的页。内核分配器维护着一个链表：kalloc()（kernel/kalloc.c:69）从空闲页链表中弹出一页内存，而kfree()（kernel/kalloc.c:47）向链表中推入一个空闲页。为了达到最佳性能，我们可能会希望两个父进程能并发执行，而不需要等待对方，但是这种想法在xv6的kfree实现下可能是不对的。

图6.1更详细地展现了整个背景：两个CPU共享链表所在的内存，并使用load和store指令操作它。（在现实中，CPU有各自的缓存，但概念上讲，多处理器系统的运行看起来就好像有一整块共享的内存。）如果没有并发请求，你可以像下面这样实现链表的push操作：

```C
struct element {
  int data;
  struct element *next;
};

struct element *list = 0;

void
push (int data)
{
  struct element *l;
  
  l = malloc(sizeof *l);
  l->data = data;
  l->next = list;
  list = l;
}
```

这种实现单独执行是没问题的。然而如果同时不止一个拷贝在执行就会出问题。如果两个CPU同时执行push，可能两边都在执行第15行，如图6.1，在两边执行16行之前，就出现了错误的输出，如图6.2所示。在之前的list值前面，next设置了两个链表元素。当16行中两边对list的赋值一旦发生，第二个将会覆盖第一个的值，而第一个中所分配的元素将会丢失。

16行所发生的丢失更新就是竞争条件的一个例子。竞争条件说的就是，当一个内存地址被并行访问且其中至少一个访问是写。竞争往往是bug的标志，可能是一次丢失的更新（如果访问都是写），也可能是读取未完全更新的数据结构。竞争的结果取决于参与的两个CPU的精确时间，以及它们的内存操作在内存系统中被安排的顺序。这都使得包含竞争的问题难以复现和调试。举例来说，当调试push时添加打印语句可能会改变执行的时间，从而导致竞争消失。

防止竞争的常用手段就是上锁。锁保证了互斥，从而使得同一时间只有一个CPU能执行push中的敏感行；这就防止了以上场景的出现。上面代码的正确上锁版本仅仅加了几行：
```C
struct element *list = 0;
struct lock listlock;

void
push(int data)
{
  struct element *l;
  l = malloc(sizeof *l);
  l->data = data;
  acquire(&listlock);
  l->next = list;
  list = l;
  release(&listlock);
}
```
acquire与release之间的指令通常被称作临界区域。上锁的动作通常称为保护链表。

~~当我们说锁保护数据的时候，我们说的其实是锁保护了数据中的一组不变量。不变量是数据结构经过操作之后还能保持不变的特性。典型地，一个操作的正确性依赖于操作开始时，不变量的正确性。这个操作中间可能会暂时改变不变量，但在操作结束前，必须将不变量恢复。比方在链表的例子中，不变量就是list指向脸变中的第一个元素，而且每个元素的next域指针指向它的下一个元素。push的调用暂时改变了这个不变量：17行中，l指向下一个链表元素，但list还没有指向l（在18行恢复）。我们之前看到的竞争条件就发生在第二个CPU执行代码中，不变量暂时被改变的情况下。锁的恰当使用要保证同一时间只有一个CPU能够在临界区内的数据结构上进行操作，这样在数据结构的不变量被改变时，就没有CPU能够执行数据结构的操作。~~

尽管正确使用锁能够让防止代码出错，但锁的加入限制了性能。举例来说，如果两个进程同时调用kfree，锁将会使两个调用串行化，我们也不会从不同CPU的运行上获得任何好处。我们把多个进程同时想要获取一把锁的情况称为冲突（conflict），或者说锁被争用（contention）。在内核设计中，避免锁争用是一个主要的挑战。xv6基本没做相关的事情，但复杂的内核都要专门组织数据结构和算法，来避免锁争用。在链表这个例子中，内核可以为每一个CPU维护一个空闲页链表，而且只有在自己的链表为空时，才会去碰别的CPU的空闲页链表，而且它必须从别的CPU的链表中偷走内存。其他的使用实例可能需要更加复杂的设计。

锁放置的位置对性能也十分重要。在我们的例子中，我们可以把acquire放到push中更靠前的位置，放到13行之前都没有问题。这样做会降低性能，因为对malloc的调用也被串行化了。后面的“锁使用”一节为在哪插入acquire和release调用提供了一些指导原则。

critical section  invariants  with respect to  properties  violate  contention

# 6.2 代码：锁
xv6有两种锁：自旋锁和睡眠锁。我们从自旋锁讲起。xv6使用spinlock结构体表示自旋锁。结构体中重要的域是locked，当锁可以获取时值为0，当锁被持有时非0。逻辑上讲，xv6应该使用以下的代码来获取锁：

```C
void
acquire(struct spinlock *lk) // does not work!
{
  for (;;) {
    if (lk->locked == 0) {
      lk->locked = 1;
      break;
    }
  }
}
```

不幸的是，这种实现无法保证多处理器下的互斥。两个CPU可能同时到达第25行，发现lk->locked是0，然后都会执行26行，获取到锁。此时，两个不同的CPU都持有了这把锁，这就违背了互斥性。我们需要一种方法来让第25、26行执行为一个原子（即无法分割）步。

因为锁被广泛使用，多处理器通常会提供指令能够实现25、26行的原子版本。在RISC-V上，这个指令时amoswap r，a.amoswap读取内存地址a的值，向那个地址写入寄存器r的内容，然后将再将读到的值放入r。就是说，它交换了寄存器和内存地址的值。它原子地执行这个序列，使用特殊的硬件来阻止其他CPU在读写中间使用这个内存地址。

xv6的`acquire`（kernel/spinlock.c:22）使用可移动的C库函数`__sync_lock_test_and_set`，函数本质上用了`amoswap`指令；返回值是lk->locked中的旧（被交换的）值。acquire函数把这个交换放在循环中，一直重试直到获取到锁。每个循环都会用1与lk->locked交换，然后检查之前的值；如果之前值为0，那我们就拿到了锁，交换也已将lk->locked设置为1。如果之前的值为1，那有其他的CPU正持有这把锁，我们将1与lk->locked交换并不会改变它的值。

一旦获取到锁，acquire记录下哪个CPU拿到了锁，这个信息用于调试。lk->cpu域被锁保护起来，只有持有锁才能修改。

release（kernel/spinlock.c:47）是aquire的反面：它清除lk->cpu域然后释放锁。概念上讲，释放锁只需要将0赋值给lk->locked。C标准允许编译器使用不同的store指令实现赋值，因此C赋值对并行代码来说可能是非原子操作。因此release使用C库函数`__sync_lock_release`，它实现了原子赋值。这个函数归根到底同样调用了一个RISC-V amoswap指令。

wraps  boils down to


