包括xv6在内的大部分内核，都交替执行不同的活动。交替执行的一个来源是多处理器硬件：计算机中有多个CPU独立执行，例如xv6的RISC-V。多CPU共享物理RAM，xv6利用共享，维护所有CPU都需要读写的数据结构。这种共享提高了一个CPU在更新一个数据结构到一半时，另一个CPU读取它的可能性。也提高了多个CPU同时更新同一个数据的可能性。没有仔细的设计，这种并行访问很可能导致错误的结果或者造成数据结构的破坏。即使是单处理器，内核也可能在不同线程之间切换CPU，导致它们的执行相互交替。最后还有一种情况，当设备终端handler与被打断的代码修改相同数据，如果中断在错误的时间出现，就可能造成数据的破坏。并行指的是多条指令流相互交替执行的情况，原因可以是多处理器、线程切换或者中断。

内核中充满了并发访问的数据。举个例子，两个CPU可以同时调用kalloc，一次同时从空闲页链表头部弹出页。内核设计者喜欢高并发，因为这样能通过并行获取更高的性能，同时也能够提升响应性能。然而，这样做的结果是，内核设计者们必须付出很多努力，来保证并发之下的正确性。有很多方法能写出正确的并发代码，有些比别的更容易理解。保证并发之下正确的策略与支持它们的抽象被称为并发控制技术。

xv6在不同的情境之下，使用多种并发控制技术；可以用的还用更多。本章主要介绍一种广泛使用的技术：锁。锁提供了互斥性（mutual exclusion, mutex就这么来的），保证在同一时间只有一个CPU持有这把锁。如果程序员将每个共享数据项关联一把锁，代码在使用一个项的时候总是持有关联的锁，那就能保证这个数据项在同一时间只被一个CPU使用。在这种情况下，我们就说锁保护了这个数据项。尽管锁是一种容易理解的并发控制机制，但它的缺点在于，它的使用会降低性能，因为从本质上说，锁把并行操作串行化了。

本章的其他部分解释了为什么xv6需要锁，xv6怎样实现锁，以及怎样使用锁。

interleave  simultaneously  concurrency  parallelism  mutual exclusion

# 6.1 竞争条件（Race Condition）

为了阐明为什么我们需要锁，我们来看一个例子，在不同CPU上运行的两个进程都调用了wait。wait释放了子进程的内存。因此，在每个CPU上，内核都会调用kfree来释放子进程的页。内核分配器维护着一个链表：kalloc()（kernel/kalloc.c:69）从空闲页链表中弹出一页内存，而kfree()（kernel/kalloc.c:47）向链表中推入一个空闲页。为了达到最佳性能，我们可能会希望两个父进程能并发执行，而不需要等待对方，但是这种想法在xv6的kfree实现下可能是不对的。

图6.1更详细地展现了整个背景：两个CPU共享链表所在的内存，并使用load和store指令操作它。（在现实中，CPU有各自的缓存，但概念上讲，多处理器系统的运行看起来就好像有一整块共享的内存。）如果没有并发请求，你可以像下面这样实现链表的push操作：

```C
struct element {
  int data;
  struct element *next;
};

struct element *list = 0;

void
push (int data)
{
  struct element *l;
  
  l = malloc(sizeof *l);
  l->data = data;
  l->next = list;
  list = l;
}
```

这种实现单独执行是没问题的。然而如果同时不止一个拷贝在执行就会出问题。如果两个CPU同时执行push，可能两边都在执行第15行，如图6.1，在两边执行16行之前，就出现了错误的输出，如图6.2所示。在之前的list值前面，next设置了两个链表元素。当16行中两边对list的赋值一旦发生，第二个将会覆盖第一个的值，而第一个中所分配的元素将会丢失。

16行所发生的丢失更新就是竞争条件的一个例子。竞争条件说的就是，当一个内存地址被并行访问且其中至少一个访问是写。竞争往往是bug的标志，可能是一次丢失的更新（如果访问都是写），也可能是读取未完全更新的数据结构。竞争的结果取决于参与的两个CPU的精确时间，以及它们的内存操作在内存系统中被安排的顺序。这都使得包含竞争的问题难以复现和调试。举例来说，当调试push时添加打印语句可能会改变执行的时间，从而导致竞争消失。

防止竞争的常用手段就是上锁。锁保证了互斥，从而使得同一时间只有一个CPU能执行push中的敏感行；这就防止了以上场景的出现。上面代码的正确上锁版本仅仅加了几行：
```C
struct element *list = 0;
struct lock listlock;

void
push(int data)
{
  struct element *l;
  l = malloc(sizeof *l);
  l->data = data;
  acquire(&listlock);
  l->next = list;
  list = l;
  release(&listlock);
}
```
acquire与release之间的指令通常被称作临界区域。上锁的动作通常称为保护链表。

~~当我们说锁保护数据的时候，我们说的其实是锁保护了数据中的一组不变量。不变量是数据结构经过操作之后还能保持不变的特性。典型地，一个操作的正确性依赖于操作开始时，不变量的正确性。这个操作中间可能会暂时改变不变量，但在操作结束前，必须将不变量恢复。比方在链表的例子中，不变量就是list指向脸变中的第一个元素，而且每个元素的next域指针指向它的下一个元素。push的调用暂时改变了这个不变量：17行中，l指向下一个链表元素，但list还没有指向l（在18行恢复）。我们之前看到的竞争条件就发生在第二个CPU执行代码过程中，不变量暂时被改变的情况下。锁的恰当使用要保证同一时间只有一个CPU能够在临界区内的数据结构上进行操作，这样在数据结构的不变量被改变时，就没有CPU能够执行数据结构的操作。~~

尽管正确使用锁能够让防止代码出错，但锁的加入限制了性能。举例来说，如果两个进程同时调用kfree，锁将会使两个调用串行化，我们也不会从不同CPU的运行上获得任何好处。我们把多个进程同时想要获取一把锁的情况称为冲突（conflict），或者说锁被争用（contention）。在内核设计中，避免锁争用是一个主要的挑战。xv6基本没做相关的事情，但复杂的内核都要专门组织数据结构和算法，来避免锁争用。在链表这个例子中，内核可以为每一个CPU维护一个空闲页链表，而且只有在自己的链表为空时，才会去碰别的CPU的空闲页链表，而且它必须从别的CPU的链表中偷走内存。其他的使用实例可能需要更加复杂的设计。

锁放置的位置对性能也十分重要。在我们的例子中，我们可以把acquire放到push中更靠前的位置，放到13行之前都没有问题。这样做会降低性能，因为对malloc的调用也被串行化了。后面的“锁使用”一节为在哪插入acquire和release调用提供了一些指导原则。

critical section  invariants  with respect to  properties  violate  contention

# 6.2 代码：锁
xv6有两种锁：自旋锁和睡眠锁。我们从自旋锁讲起。xv6使用spinlock结构体表示自旋锁。结构体中重要的域是locked，当锁可以获取时值为0，当锁被持有时非0。逻辑上讲，xv6应该使用以下的代码来获取锁：

```C
void
acquire(struct spinlock *lk) // does not work!
{
  for (;;) {
    if (lk->locked == 0) {
      lk->locked = 1;
      break;
    }
  }
}
```

不幸的是，这种实现无法保证多处理器下的互斥。两个CPU可能同时到达第25行，发现lk->locked是0，然后都会执行26行，获取到锁。此时，两个不同的CPU都持有了这把锁，这就违背了互斥性。我们需要一种方法来让第25、26行执行为一个原子（即无法分割）步。

因为锁被广泛使用，多处理器通常会提供指令能够实现25、26行的原子版本。在RISC-V上，这个指令时amoswap r，a.amoswap读取内存地址a的值，向那个地址写入寄存器r的内容，然后将再将读到的值放入r。就是说，它交换了寄存器和内存地址的值。它原子地执行这个序列，使用特殊的硬件来阻止其他CPU在读写中间使用这个内存地址。

xv6的`acquire`（kernel/spinlock.c:22）使用可移动的C库函数`__sync_lock_test_and_set`，函数本质上用了`amoswap`指令；返回值是lk->locked中的旧（被交换的）值。acquire函数把这个交换放在循环中，一直重试直到获取到锁。每个循环都会用1与lk->locked交换，然后检查之前的值；如果之前值为0，那我们就拿到了锁，交换也已将lk->locked设置为1。如果之前的值为1，那有其他的CPU正持有这把锁，我们将1与lk->locked交换并不会改变它的值。

一旦获取到锁，acquire会记录哪个CPU拿到了锁，这个信息用于调试。lk->cpu域被锁保护起来，只有持有锁才能修改。

release（kernel/spinlock.c:47）是aquire的反面：它清除lk->cpu域然后释放锁。概念上讲，释放锁只需要将0赋值给lk->locked。C标准允许编译器使用不同的store指令实现赋值，因此C赋值对并行代码来说可能是非原子操作。因此release使用C库函数`__sync_lock_release`，它实现了原子赋值。这个函数归根到底同样调用了一个RISC-V amoswap指令。

wraps  boils down to

# 6.3 代码：使用锁

xv6在很多地方使用锁来避免竞争条件。如上所述，kalloc（kernel/kalloc.c:69）和kfree（kenel/kalloc.c:47）是很好的例子。尝试下练习1和2，看看如果把这些函数中的锁删掉会发生什么。你可能会发现，触发错误行为非常之难，这也说明想要可靠地测试代码是否有锁错误和竞争条件非常困难。xv6中很可能也有一些竞争。

锁的使用中困难的部分在于，怎样确定要用多少锁，每把锁要保护什么数据和不变量。这里有一些基本原则：首先，不论何时，如果一个CPU在写入一个变量时，另一个CPU可以读或者写它，我们就需要一把锁来隔离这两种操作，防止重叠。第二，记住锁保护不变量：如果不变量包括多个内存地址，通常，所有地址需要被一把锁保护起来，以保证不变量能够保持。

上面的原则说明了何时必须用锁，但没说何时不必用锁。实际上少用锁对提升效率非常重要，因为锁会减少并行度。如果并发不重要，那我们可以使用单个线程，不需要考虑锁。一个简单的内核可以在多处理器机器上这样做：使用一把锁保护内核，在进入内核时必须获取锁，离开内核时释放锁（尽管管道read等wait等系统调用会出问题）。很多单处理器系统通过这种方式迁移到多处理器上，有时被称作“大内核锁”，但这种方法牺牲了并行能力：同一时间只有一个CPU可以在内核执行。如果内核需要进行大量的运算，使用大量细粒度的锁能够提升效率，因为这样内核就能在多个CPU上同时执行。

作为粗粒度锁的一个例子，xv6的kalloc.c分配器使用单个锁保护的单个空闲页链表。如果不同CPU上的不同进程同时分配页，每一个都必须通过在acquire中自旋来等待自己的顺序。自旋做的是无用的工作，所以会降低性能。如果锁争用明显浪费了大量CPU时间，可能可以通过改变分配器的设计来提升性能，将分配器改为多个空闲页链表，每个链表都有自己锁，从而允许真正的并行分配。

作为细粒度锁的例子，xv6为每个文件分别设置了锁，这样操作不同文件的进程就可以正常执行，而不需要等待其他进程的锁。文件锁组合还可以做得更精细，如果需要允许不同进程同时修改文件的不同部分的话。总之，锁的粒度大小，需要综合考量性能与复杂度。

后面的章节介绍了xv6的各个部分，它们将给出xv6使用锁处理并发的实例。作为预告，图6.3列出了xv6中的所有锁。

suggesting   fine-grained   scheme  preview

# 6.4 死锁和锁排序

如果内核中一段代码的执行路径上必须持同时有多把锁，那么所有代码路径都应该按相同的顺序获取这些锁。否则就会有死锁的可能。如果xv6中有两条代码路径都需要锁A和锁B，但代码路径1以先A后B的顺序获取锁，路径2先B后A。假设线程T1执行代码路径1，获取了锁A，线程T2执行代码路径2，获取了锁B。然后T1要获取锁B，而T2要获取锁A。两边都持有对方需要的锁，aquire不返回就无法释放。这样，两个线程都将永久阻塞。为了避免这样的死锁，所有代码路径都应该按相同的顺序获取锁。对全局锁获取顺序的需求意味着锁是每个函数使用规范的有效组成部分：调用者必须以一种使锁以商定的顺序获得的方式调用函数。

由于sleep的工作方式（参见第7章），xv6有很多包含了进程锁（proc结构体中的锁）的长度为2的锁顺序链。举例来说，consoleintr（kernel/console.c:138）是处理打字输入中断的例行程序。当新行到来时，每个等待控制台输入的进程都要被唤醒。consoleintr持有cons.lock，然后调用wakeup函数。wakeup获取等待中进程的锁来唤醒它。因此，全局防止死锁的锁顺序规则包括一条：cons.lock必须在任何进程锁获取之前获取。文件系统代码包含了xv6中最长的锁顺序链。举例来说，创建一个文件需要同时持有所在目录的锁，新文件inode的锁，硬盘块缓冲区的锁，硬盘驱动的vdisk_lock，以及调用进程的p->lock。为防止死锁，文件系统代码通常按照上面所提顺序获取锁。

遵循全局避免死锁顺序有时出奇的难。有时锁顺序与程序结构的逻辑顺序冲突，例如，代码模块M1调用M2，但锁顺序需要先获取M2中的锁，后获取M1中的锁。有时锁的身份并不能提前知道，可能因为持有一把锁时，才能知道获取的下一把锁的身份。这种情况在文件系统查找一个路径下连续的部分时出现，也会在wait与exit的代码中出现，当它们在进程表中查找子进程时。总之，因为锁的数量越多，死锁的可能性越大，死锁的危险限制了锁方案中锁的精细程度。在操作系统实现中，避免死锁的需求经常是需要考虑的重要因素。

indefinitely  routine  Finally  major

# 6.5 锁和中断handler

有些xv6自旋锁保护线程和中断处理器都是用的数据。举例来说，clockintr计时器中断处理器可能增加ticks（kernel/trap.c:163），几乎与此同时，内核线程在sys_leep（kenel/sysproc.c:64）中读取ticks。ticklock将这两个访问串行化。

自旋锁和中断的相互作用引起了潜在的危险。假设sys_sleep持有tickslock，它的CPU被计时器中断所打断。clockintr会试着获取tickslock，发现它被持有，等待它被释放。这种情况下，tickslock将永远不被释放：只有sys_sleep能释放它，但sys_sleep在clockintr返回之前无法继续执行。所以CPU将死锁，任何需要这把锁的代码也都将被冻结。

为避免这种情况，如果自旋锁被中断处理器使用，CPU不允许在终端开启的使用它。xv6更加传统：当CPU获取到任何锁时xv6都将禁止那个CPU上的中断。其他CPU仍然可被中断，所以中断的acquire可以等待线程释放自旋锁，只要不是同一个CPU。

当CPU未持有自旋锁时，会重新开启中断；它必须做一点簿记的工作来处理嵌套的临界区。acquire调用push_off（kernel/spinlock.c:89）而release调用pop_off（kernel/spinlock.c:100）来记录当前CPU的锁嵌套层数。当这个数边为0时，pop_off恢复中断允许状态，它位于最外层临界区的开头。intr_off与intr_on函数分别执行RISC-V的关闭和开启中断的指令。

acquire严格地在设置lk->locked（kernel/spinlock.c:28）之前调用push_off。如果调换顺序，将有一段很短的时间，持有锁的同时，中断打开；这时如果不幸地发生了一个中断，系统就会死锁。同样，release也必须在释放锁之后调用pop_off（kernel/spinlock.c:66）。

increment  interaction  cope with nested critical sections

# 6.6 指令和内存排序

人们会自然而然地认为程序是按照源代码中的语句顺序执行。然而，很多编译器和CPU为了获得更高的性能，都会打乱代码执行的顺序。如果一条指令需要许多周期才能完成，CPU可能会提前执行该指令，这样它就可以与其他指令重叠，避免CPU停止运行。举个例子，CPU可能注意到在一个串行序列中，指令A与指令B不相互依赖。CPU可能先执行指令B，可能是因为它的输入比指令A更早准备好，也可能是为了将A与B的执行重叠起来。编译器可以执行类似的重新排序，它可以在源文件中一个语句的指令之前发出另一个它后面语句的指令。

编译器和CPU重排序的时候会收一条规则，不能改变正确书写的串行代码的运行结果。然而，规则允许重排序改变并行代码的执行结果，这很容易导致多处理器上的错误行为[2,3]。CPU的排序规则被称为内存模型。

举个例子，在push中的这段代码中，如果编译器或CPU将第4行的store操作移动到第6行的release后面，将会引发严重后果：

```C
l = malloc(sizeof *l);
l->data = data;
acquire(&listlock);
l->next = list;
list = l;
release(&listlock);
```

如果这样的重排序出现，有一段时间另一个CPU可以获取锁并查看更新后的list，但只能看到未初始化的list->next。

为了让硬件和编译器不进行重排序，xv6在acquire（kernel/spinlock.c:22）与release（kernel/spinlock.c:47）中使用__sync_syncronize()。__sync_synchronize()是一个内存屏障：它告诉编译器与CPU不要重排序屏障内部的加载或保存操作。xv6中acquire与release的屏障在与它相关的几乎所有情况中都强制指定顺序，因为xv6使用锁来保护共享数据的访问。第9章会讨论一些例外情况。

stalls  precedes  barrier

# 6.7 睡眠锁

有时，xv6需要长时间持有锁。例如，文件系统（第8章）在读写一个文件在硬盘上的内容时，会将文件上锁，这些硬盘操作可能持续几十毫秒。如果是内旋锁，如果有别的进程想获取锁的话，持有这么久会造成浪费，因为自旋锁中获取过程会浪费很久的CPU。

retain

